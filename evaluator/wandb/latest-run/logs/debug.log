2024-03-27 18:55:19,484 INFO    MainThread:27424 [wandb_setup.py:_flush():76] Current SDK version is 0.16.4
2024-03-27 18:55:19,484 INFO    MainThread:27424 [wandb_setup.py:_flush():76] Configure stats pid to 27424
2024-03-27 18:55:19,484 INFO    MainThread:27424 [wandb_setup.py:_flush():76] Loading settings from /Users/vishwassetty/.config/wandb/settings
2024-03-27 18:55:19,484 INFO    MainThread:27424 [wandb_setup.py:_flush():76] Loading settings from /Users/vishwassetty/PycharmProjects/example/wandb/settings
2024-03-27 18:55:19,484 INFO    MainThread:27424 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2024-03-27 18:55:19,484 INFO    MainThread:27424 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2024-03-27 18:55:19,484 INFO    MainThread:27424 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'test_llm.py', 'program_abspath': '/Users/vishwassetty/PycharmProjects/example/test_llm.py', 'program': '/Users/vishwassetty/PycharmProjects/example/test_llm.py'}
2024-03-27 18:55:19,485 INFO    MainThread:27424 [wandb_init.py:_log_setup():526] Logging user logs to /Users/vishwassetty/PycharmProjects/example/wandb/run-20240327_185519-neez3a9l/logs/debug.log
2024-03-27 18:55:19,485 INFO    MainThread:27424 [wandb_init.py:_log_setup():527] Logging internal logs to /Users/vishwassetty/PycharmProjects/example/wandb/run-20240327_185519-neez3a9l/logs/debug-internal.log
2024-03-27 18:55:19,485 INFO    MainThread:27424 [wandb_init.py:init():566] calling init triggers
2024-03-27 18:55:19,485 INFO    MainThread:27424 [wandb_init.py:init():573] wandb.init called with sweep_config: {}
config: {'adafactor_beta1': None, 'adafactor_clip_threshold': 1.0, 'adafactor_decay_rate': -0.8, 'adafactor_eps': (1e-30, 0.001), 'adafactor_relative_step': True, 'adafactor_scale_parameter': True, 'adafactor_warmup_init': True, 'adam_betas': (0.9, 0.999), 'adam_epsilon': 1e-08, 'best_model_dir': 'outputs/bert/best_model', 'cache_dir': 'cache_dir/', 'config': {}, 'cosine_schedule_num_cycles': 0.5, 'custom_layer_parameters': [], 'custom_parameter_groups': [], 'dataloader_num_workers': 0, 'do_lower_case': False, 'dynamic_quantize': False, 'early_stopping_consider_epochs': False, 'early_stopping_delta': 0, 'early_stopping_metric': 'correct', 'early_stopping_metric_minimize': False, 'early_stopping_patience': 3, 'encoding': None, 'eval_batch_size': 64, 'evaluate_during_training': True, 'evaluate_during_training_silent': True, 'evaluate_during_training_steps': 1000, 'evaluate_during_training_verbose': False, 'evaluate_each_epoch': True, 'fp16': False, 'gradient_accumulation_steps': 1, 'learning_rate': 4e-05, 'local_rank': -1, 'logging_steps': 50, 'loss_type': None, 'loss_args': {}, 'manual_seed': None, 'max_grad_norm': 1.0, 'max_seq_length': 128, 'model_name': 'bert-base-cased', 'model_type': 'bert', 'multiprocessing_chunksize': -1, 'n_gpu': 1, 'no_cache': False, 'no_save': False, 'not_saved_args': [], 'num_train_epochs': 10, 'optimizer': 'AdamW', 'output_dir': 'outputs/bert', 'overwrite_output_dir': True, 'polynomial_decay_schedule_lr_end': 1e-07, 'polynomial_decay_schedule_power': 1.0, 'process_count': 6, 'quantized_model': False, 'reprocess_input_data': True, 'save_best_model': True, 'save_eval_checkpoints': False, 'save_model_every_epoch': False, 'save_optimizer_and_scheduler': True, 'save_steps': 2000, 'scheduler': 'linear_schedule_with_warmup', 'silent': False, 'skip_special_tokens': True, 'tensorboard_dir': None, 'thread_count': None, 'tokenizer_name': None, 'tokenizer_type': None, 'train_batch_size': 128, 'train_custom_parameters_only': False, 'trust_remote_code': False, 'use_cached_eval_features': True, 'use_early_stopping': False, 'use_hf_datasets': False, 'use_multiprocessing': True, 'use_multiprocessing_for_evaluation': True, 'wandb_kwargs': {'name': 'bert-base-cased'}, 'wandb_project': 'Question Answer Application', 'warmup_ratio': 0.06, 'warmup_steps': 10, 'weight_decay': 0.0, 'model_class': 'QuestionAnsweringModel', 'doc_stride': 384, 'lazy_loading': False, 'max_answer_length': 100, 'max_query_length': 64, 'n_best_size': 3, 'null_score_diff_threshold': 0.0, 'special_tokens_list': []}
2024-03-27 18:55:19,485 INFO    MainThread:27424 [wandb_init.py:init():616] starting backend
2024-03-27 18:55:19,485 INFO    MainThread:27424 [wandb_init.py:init():620] setting up manager
2024-03-27 18:55:19,486 INFO    MainThread:27424 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
2024-03-27 18:55:19,489 INFO    MainThread:27424 [wandb_init.py:init():628] backend started and connected
2024-03-27 18:55:19,507 INFO    MainThread:27424 [wandb_init.py:init():720] updated telemetry
2024-03-27 18:55:19,507 INFO    MainThread:27424 [wandb_init.py:init():753] communicating run to backend with 90.0 second timeout
2024-03-27 18:55:21,127 INFO    MainThread:27424 [wandb_run.py:_on_init():2262] communicating current version
2024-03-27 18:55:21,291 INFO    MainThread:27424 [wandb_run.py:_on_init():2271] got version response upgrade_message: "wandb version 0.16.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2024-03-27 18:55:21,291 INFO    MainThread:27424 [wandb_init.py:init():804] starting run threads in backend
2024-03-27 18:55:21,435 INFO    MainThread:27424 [wandb_run.py:_console_start():2241] atexit reg
2024-03-27 18:55:21,435 INFO    MainThread:27424 [wandb_run.py:_redirect():2096] redirect: wrap_raw
2024-03-27 18:55:21,435 INFO    MainThread:27424 [wandb_run.py:_redirect():2161] Wrapping output streams.
2024-03-27 18:55:21,435 INFO    MainThread:27424 [wandb_run.py:_redirect():2186] Redirects installed.
2024-03-27 18:55:21,435 INFO    MainThread:27424 [wandb_init.py:init():847] run started, returning control to user process
2024-03-27 18:55:21,436 INFO    MainThread:27424 [wandb_watch.py:watch():51] Watching
2024-03-27 21:07:05,203 WARNING MsgRouterThr:27424 [router.py:message_loop():77] message_loop has been closed
